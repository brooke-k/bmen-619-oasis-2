{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import nibabel as nib\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/10 (90.00%)[██████████████████--]\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "def printUpdate(count, total):\n",
    "  clear_output(wait=True)\n",
    "  percent = 100.*count/total\n",
    "  print(f\"{count}/{total} ({percent:.2f}%)\", end=\"\")\n",
    "  print(f'[{\"██\"*int(percent//10)}{\"--\"*int((100-percent)//10)}]')\n",
    "\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "  printUpdate(i, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveRename(source, dest, num_files = 291):\n",
    "    \"\"\"\n",
    "    Moves all .hdr and .img images from source location to a single directory\n",
    "    \"\"\"\n",
    "    rawDirPat = r\"(?:[\\W\\S]+?)OAS2_([0-9]{4})_MR([0-9]{1})/RAW\"\n",
    "    i = 0\n",
    "    for root, dir, files in os.walk(source):\n",
    "        r_match = re.findall(rawDirPat, root)\n",
    "        if len(r_match) > 0:\n",
    "            subID = r_match[0][0]\n",
    "            session = r_match[0][1]\n",
    "            new_name = f\"{subID}_{session}\"\n",
    "            for f in files:\n",
    "                fname, fext = os.path.splitext(f)\n",
    "                if fext == \".img\" or fext == \".hdr\":\n",
    "                    f_match = re.findall(r\"mpr-([0-9]{1}).nifti\", fname)\n",
    "                    if len(f_match) > 0:\n",
    "                        f_num = f_match[0]\n",
    "                        old_name = os.path.join(root, f)\n",
    "                        new_name = os.path.join(\n",
    "                            dest, f\"OAS2_{subID}_MR{f_num}_V{session}.nifti{fext}\"\n",
    "                        )\n",
    "\n",
    "                        printUpdate(i, num_files)\n",
    "                        i += 1\n",
    "                        shutil.copy2(old_name, new_name)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToNii(source, num_files = 291):\n",
    "    \"\"\"\n",
    "    Convert images from .img to .nii format and get rid of .img and .hdr files.\n",
    "    \"\"\"\n",
    "\n",
    "    i = 0\n",
    "    for root, dir, files in os.walk(source):\n",
    "        for f in files:\n",
    "            fbase, fext = os.path.splitext(f)\n",
    "            if fext == \".img\":\n",
    "                # print(f\"Converting {f}\")\n",
    "                printUpdate(i, num_files)\n",
    "                i += 1\n",
    "                fname = os.path.join(root, f)\n",
    "                img = nib.load(fname)\n",
    "                nib.save(img, fname.replace(\".img\", \".nii\"))\n",
    "                os.remove(os.path.join(root, fbase + \".hdr\"))\n",
    "                os.remove(os.path.join(root, fbase + \".img\"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "\n",
    "def toRGB(data):\n",
    "    x, y = data.shape[:2]\n",
    "    data = (data-data.min())/(data.max()-data.min())\n",
    "\n",
    "    img_arr = np.empty(shape=(x,y,4))\n",
    "    img_arr[:, :, :3] = data\n",
    "    img_arr[:, :, 3] = 1.\n",
    "    return img_arr\n",
    "\n",
    "def convertToJPG(source, num_files = 291, transverse_slice = 116):\n",
    "    \"\"\"\n",
    "    Convert images from .nii to .jpg format\n",
    "\n",
    "    i = 0\n",
    "    shape = (0,0,0,0)\n",
    "    for root, dir, files in os.walk(source):\n",
    "        for f in files:\n",
    "            fbase, fext = os.path.splitext(f)\n",
    "            if fext == \".nii\":\n",
    "                # print(f\"Converting {f}\")\n",
    "                printUpdate(i, num_files)\n",
    "                i += 1\n",
    "                fname = os.path.join(root, f)\n",
    "                img = nib.load(fname)\n",
    "                data = img.get_fdata()[transverse_slice,:,:]\n",
    "\n",
    "                img_arr = toRGB(data)\n",
    "                shape = img_arr.shape\n",
    "                plt.imsave(fname.replace(\".nifti.nii\", \".jpg\"), img_arr)\n",
    "                # os.remove(os.path.join(root, fbase + \".nii\"))\n",
    "    printUpdate(num_files, num_files)\n",
    "\n",
    "    return shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeVisits3to5(source):\n",
    "    \"\"\"\n",
    "    Removes all images taken at visits 3-5\n",
    "    \"\"\"\n",
    "    for root, dir, files in os.walk(source):\n",
    "        for f in files:\n",
    "            m = re.match(r\"OAS2_[0-9]{4}_MR[0-9]{1}_V([0-9]{1})\", f)\n",
    "            session_num = int(m.groups()[0])\n",
    "            if session_num > 2:\n",
    "                print(f\"Removing {f}\")\n",
    "                os.remove(os.path.join(root, f))\n",
    "\n",
    "def removeSubjects(subjectIDs:list, source):\n",
    "    for root, dir, files in os.walk(source):\n",
    "        for f in files:\n",
    "            for id in subjectIDs:\n",
    "                m = re.match(re.compile(\"(\"+id+\")\"), f)\n",
    "                if m!= None:\n",
    "                    subjectIDs.remove(id)\n",
    "                    os.remove(os.path.join(root, f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run once, to move data\n",
    "# Set True to move files\n",
    "move_files = False\n",
    "convert_files = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "if move_files:\n",
    "  moveRename(\"datasets/OAS2\", \"datasets/OAS2_nii\", 2733)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "if move_files:\n",
    "  removeVisits3to5(\"datasets/OAS2_nii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if move_files:\n",
    "  convertToNii(\"datasets/OAS2_nii\", 1366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove subject MRIs who had an age > 95\n",
    "if move_files:\n",
    "  removeSubjects([\"OAS2_0051_MR3\", \"OAS2_0087_MR1\", \"OAS2_0087_MR2\"], os.getenv(\"OAS2NII\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveByClass(df, num_files = 291, removeOriginal=True):\n",
    "  train_path = os.path.join(os.getenv(\"OAS2NII\"), \"train\")\n",
    "  test_path = os.path.join(os.getenv(\"OAS2NII\"), \"test\")\n",
    "  validate_path = os.path.join(os.getenv(\"OAS2NII\"), \"validate\")\n",
    "\n",
    "  data_paths = [train_path, test_path, validate_path]\n",
    "  for path in data_paths:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)\n",
    "      if not os.path.exists(os.path.join(path, \"class_1\")):\n",
    "        os.makedirs(os.path.join(path, \"class_1\"))\n",
    "      if not os.path.exists(os.path.join(path, \"class_0\")):\n",
    "        os.makedirs(os.path.join(path, \"class_0\"))\n",
    "\n",
    "  new_paths = []\n",
    "  i = 0\n",
    "  for index, row in df.iterrows():\n",
    "    old_name = row[\"file\"]\n",
    "    fname = os.path.split(old_name)[1]\n",
    "    if row[\"Group\"] == 0:\n",
    "      new_name = os.path.join(os.getenv(\"OAS2NII\"), row[\"Split\"], \"class_0\", fname)\n",
    "    else:\n",
    "      new_name = os.path.join(os.getenv(\"OAS2NII\"), row[\"Split\"], \"class_1\", fname)\n",
    "    new_paths += [new_name]\n",
    "    printUpdate(i, num_files)\n",
    "    i += 1\n",
    "    if os.path.exists(old_name):\n",
    "      os.rename(old_name, new_name)\n",
    "    print(f'Old: {old_name}\\nNew:{new_name}\\n')\n",
    "\n",
    "  files = [f for f in os.listdir(os.getenv(\"OAS2NII\")) if os.path.isfile(os.path.join(os.getenv(\"OAS2NII\"), f))]\n",
    "  if removeOriginal:\n",
    "    for f in files:\n",
    "      if os.path.exists(os.path.join(os.getenv(\"OAS2NII\"), f)):\n",
    "        os.remove(os.path.join(os.getenv(\"OAS2NII\"),f))\n",
    "\n",
    "  df[\"file\"] = new_paths\n",
    "  return df.copy(deep=True)\n",
    "\n",
    "def moveBackToSource(df, deleteInstead=False):\n",
    "  for root, dir, files in os.walk(os.getenv(\"OAS2NII\")):\n",
    "    if root!=os.getenv(\"OAS2NII\"):\n",
    "      for f in files:\n",
    "        if deleteInstead:\n",
    "          os.remove(os.path.join(root, f))\n",
    "        else:\n",
    "          os.rename(os.path.join(root, f), os.path.join(os.getenv(\"OAS2NII\"), f))\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    old_name = row[\"file\"]\n",
    "    fname = os.path.split(old_name)[-1]\n",
    "    new_name = os.path.join(os.getenv(\"OAS2NII\"), fname)\n",
    "    df.loc[index, \"file\"] = new_name\n",
    "\n",
    "  return df.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "df = pd.read_excel(\"OAS2-normalized.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "\n",
    "df.set_index(\"MRI ID\", inplace=True)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "  fname = index + \"_V\" + str(row[\"Visit\"]+1) +  \".jpg\"\n",
    "  fnames += [os.path.join(os.getenv(\"OAS2NII\"),fname)]\n",
    "\n",
    "df[\"file\"] = fnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convert_files:\n",
    "  data_shape = convertToJPG(\"datasets/OAS2_nii\", 1366, transverse_slice=134)\n",
    "else:\n",
    "  data_shape = (256, 128, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def makeTVTSplit(df):\n",
    "  x_train, x_temp, y_train, y_temp = train_test_split(df.drop(columns=[\"Group\"]), df[\"Group\"], test_size=0.2, stratify=df[['Group',\"Sex_F\"]])\n",
    "\n",
    "  strat = pd.DataFrame(x_temp)\n",
    "  strat[\"Group\"] = y_temp\n",
    "\n",
    "  x_test, x_val, y_test, y_val = train_test_split(x_temp, y_temp, stratify=strat[['Group',\"Sex_F\"]],test_size=0.2)\n",
    "\n",
    "  # df_new = moveBackToSource(df, deleteInstead=True)\n",
    "  train = x_train.copy(deep=True)\n",
    "  train[\"Split\"] = [\"train\"]*train.shape[0]\n",
    "  train[\"Group\"] = y_train.values\n",
    "\n",
    "  validate = x_val.copy(deep=True)\n",
    "  validate[\"Split\"] = [\"validate\"]*validate.shape[0]\n",
    "  validate[\"Group\"] = y_val.values\n",
    "\n",
    "  test = x_test.copy(deep=True)\n",
    "  test[\"Split\"] = [\"test\"]*test.shape[0]\n",
    "  test[\"Group\"] = y_test.values\n",
    "\n",
    "  print(f'\\tTest: {len(test)}')\n",
    "  print(f'\\tTrain: {len(train)}')\n",
    "  print(f'\\tValidate: {len(validate)}')\n",
    "\n",
    "  df_new = pd.merge(train, test, how=\"outer\")\n",
    "  df_new = pd.merge(df_new, validate, how=\"outer\")\n",
    "\n",
    "  # df_new = moveByClass(df_new, removeOriginal=True)\n",
    "\n",
    "  return df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/291 (99.66%)[██████████████████]\n",
      "Old: datasets/OAS2_nii/OAS2_0186_MR2_V2.jpg\n",
      "New:datasets/OAS2_nii/train/class_0/OAS2_0186_MR2_V2.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = makeTVTSplit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed to add cuda to path for GPU utilization\n",
    "# !source ~/.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras import ops\n",
    "import SimpleITK as sitk\n",
    "from keras import Sequential\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_base = keras.applications.ResNet50(\n",
    "  include_top=False,\n",
    "  weights='imagenet',\n",
    "  input_shape=(data_shape[0], data_shape[1], 3)\n",
    ")\n",
    "\n",
    "resnet_base.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "  resnet_base,\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(384, activation='relu'),\n",
    "  layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "  optimizer=keras.optimizers.Adam(),\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeKerasTVTDatasets(num_epochs, df, resplit=True):\n",
    "  if resplit:\n",
    "    df_new = makeTVTSplit(df)\n",
    "  else:\n",
    "    df_new = df.copy(deep=True)\n",
    "\n",
    "  dset_train = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(os.getenv(\"OAS2NII\"),\"train\"),\n",
    "    # validation_split=0.2,\n",
    "    # subset=\"training\",\n",
    "    seed=73,\n",
    "    image_size=data_shape[:2],\n",
    "    batch_size=32,\n",
    "    label_mode='binary')\n",
    "\n",
    "  dset_validate = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=os.path.join(os.getenv(\"OAS2NII\"),\"validate\"),\n",
    "    # validation_split=0.5,\n",
    "    # subset=\"validation\",\n",
    "    seed=73,\n",
    "    image_size=data_shape[:2],\n",
    "    batch_size=32,\n",
    "    label_mode='binary')\n",
    "\n",
    "  return dset_train, dset_validate, df_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/291 (99.66%)[██████████████████]\n",
      "Old: datasets/OAS2_nii/train/class_0/OAS2_0186_MR2_V2.jpg\n",
      "New:datasets/OAS2_nii/train/class_0/OAS2_0186_MR2_V2.jpg\n",
      "\n",
      "Found 204 files belonging to 2 classes.\n",
      "Found 20 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "dset_train, dset_validate, df = makeKerasTVTDatasets(50, df, resplit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 518ms/step - accuracy: 0.5183 - loss: 69.7995"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 16:23:08.052068: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1694', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-04-04 16:23:08.252432: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1694', 200 bytes spill stores, 200 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.5179 - loss: 69.5989 - val_accuracy: 0.6000 - val_loss: 24.9426\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5230 - loss: 17.4938 - val_accuracy: 0.4000 - val_loss: 12.9499\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.5364 - loss: 5.9287 - val_accuracy: 0.6000 - val_loss: 5.9390\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.6747 - loss: 2.6385 - val_accuracy: 0.5500 - val_loss: 3.8717\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8172 - loss: 1.0606 - val_accuracy: 0.6000 - val_loss: 2.8120\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.8428 - loss: 0.7944 - val_accuracy: 0.6000 - val_loss: 2.2469\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9127 - loss: 0.2949 - val_accuracy: 0.6000 - val_loss: 1.7764\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9482 - loss: 0.1199 - val_accuracy: 0.6500 - val_loss: 2.0248\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9836 - loss: 0.0349 - val_accuracy: 0.6500 - val_loss: 2.0959\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.9823 - loss: 0.0302 - val_accuracy: 0.7000 - val_loss: 2.0844\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.9988 - loss: 0.0044 - val_accuracy: 0.6000 - val_loss: 2.4523\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 0.6000 - val_loss: 2.1715\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0014 - val_accuracy: 0.5500 - val_loss: 2.1281\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.5500 - val_loss: 2.1339\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0015 - val_accuracy: 0.6500 - val_loss: 2.1468\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.6000 - val_loss: 2.1677\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.6000 - val_loss: 2.1928\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 0.6000 - val_loss: 2.2007\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.8069e-04 - val_accuracy: 0.6000 - val_loss: 2.1916\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 0.0012 - val_accuracy: 0.6000 - val_loss: 2.1749\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 9.3050e-04 - val_accuracy: 0.6000 - val_loss: 2.1602\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.0683e-04 - val_accuracy: 0.6000 - val_loss: 2.1526\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.3088e-04 - val_accuracy: 0.6000 - val_loss: 2.1482\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 9.2546e-04 - val_accuracy: 0.6000 - val_loss: 2.1497\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 8.0124e-04 - val_accuracy: 0.6000 - val_loss: 2.1477\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 8.4546e-04 - val_accuracy: 0.6000 - val_loss: 2.1463\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.5239e-04 - val_accuracy: 0.6500 - val_loss: 2.1399\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.4430e-04 - val_accuracy: 0.6500 - val_loss: 2.1358\n",
      "Epoch 29/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.3651e-04 - val_accuracy: 0.6500 - val_loss: 2.1334\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.2275e-04 - val_accuracy: 0.6500 - val_loss: 2.1363\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.9121e-04 - val_accuracy: 0.6500 - val_loss: 2.1342\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.6572e-04 - val_accuracy: 0.6500 - val_loss: 2.1323\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.6540e-04 - val_accuracy: 0.6500 - val_loss: 2.1289\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.3294e-04 - val_accuracy: 0.6500 - val_loss: 2.1310\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.0092e-04 - val_accuracy: 0.6500 - val_loss: 2.1305\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.4114e-04 - val_accuracy: 0.6500 - val_loss: 2.1272\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.8937e-04 - val_accuracy: 0.6500 - val_loss: 2.1266\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 7.2324e-04 - val_accuracy: 0.6500 - val_loss: 2.1257\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.7901e-04 - val_accuracy: 0.6500 - val_loss: 2.1246\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 7.1609e-04 - val_accuracy: 0.6000 - val_loss: 2.1219\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.1230e-04 - val_accuracy: 0.6500 - val_loss: 2.1212\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.7168e-04 - val_accuracy: 0.6500 - val_loss: 2.1243\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.5192e-04 - val_accuracy: 0.6500 - val_loss: 2.1215\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.7978e-04 - val_accuracy: 0.6500 - val_loss: 2.1232\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 6.3734e-04 - val_accuracy: 0.6000 - val_loss: 2.1223\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.6969e-04 - val_accuracy: 0.6500 - val_loss: 2.1226\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 5.3203e-04 - val_accuracy: 0.6000 - val_loss: 2.1205\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.8204e-04 - val_accuracy: 0.6000 - val_loss: 2.1174\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 5.7714e-04 - val_accuracy: 0.6000 - val_loss: 2.1181\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 1.0000 - loss: 6.2546e-04 - val_accuracy: 0.6000 - val_loss: 2.1183\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dset_train, validation_data=dset_validate, epochs=50)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"oas2-model-t134.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelPredictions(model, df, columnName=\"CM_Val\"):\n",
    "  prediction = []\n",
    "  df_test = df[df[\"Split\"]==\"test\"].copy(deep=True)\n",
    "\n",
    "  for index, row in df_test.iterrows():\n",
    "    img = keras.preprocessing.image.load_img(row[\"file\"], target_size=data_shape[:2])\n",
    "    img_arr = keras.preprocessing.image.img_to_array(img)\n",
    "    img_arr = tf.expand_dims(img_arr, 0)\n",
    "\n",
    "    prediction += [np.argmax(model.predict(img_arr))]\n",
    "\n",
    "  df_test.loc[:,\"Prediction\"] = prediction\n",
    "  cm_val = []\n",
    "  for index, row in df_test.iterrows():\n",
    "    g = int(row[\"Group\"])\n",
    "    p = int(row[\"Prediction\"])\n",
    "    if g == p:\n",
    "      if g == 0:\n",
    "        cm_val += [\"TN\"]\n",
    "      else:\n",
    "        cm_val += [\"TP\"]\n",
    "    else:\n",
    "      if g == 0:\n",
    "        cm_val += [\"FP\"]\n",
    "      else:\n",
    "        cm_val += [\"FN\"]\n",
    "\n",
    "  df_test[columnName] = cm_val\n",
    "  return df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCMV(prediction):\n",
    "  cmv = prediction.value_counts()\n",
    "  for m in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "    if m not in cmv.index:\n",
    "      cmv[m] = 0\n",
    "  return cmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrecision(cm_val:pd.Series):\n",
    "  vals = cm_val.value_counts()\n",
    "  for m in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "    if m not in vals.index:\n",
    "      vals[m] = 0\n",
    "  return float(vals[\"TP\"])/(vals[\"TP\"]+vals[\"FP\"])\n",
    "\n",
    "def getAccuracy(cm_val:pd.Series):\n",
    "  vals = cm_val.value_counts()\n",
    "  for m in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "    if m not in vals.index:\n",
    "      vals[m] = 0\n",
    "  return float(vals[\"TP\"] + vals[\"TN\"])/(vals[\"TP\"]+vals[\"FP\"]+vals[\"TN\"]+vals[\"FN\"])\n",
    "\n",
    "\n",
    "def getRecall(cm_val:pd.Series):\n",
    "  vals = cm_val.value_counts()\n",
    "  for m in [\"TP\", \"TN\", \"FP\", \"FN\"]:\n",
    "    if m not in vals.index:\n",
    "      vals[m] = 0\n",
    "  return float(vals[\"TP\"])/(vals[\"TP\"]+vals[\"TN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printModelMetrics(df_test, modelName=\"\"):\n",
    "  width = 64\n",
    "  liner = \"-\"*width\n",
    "  print(liner)\n",
    "  print(f'{\"Precision:\":<20}{getPrecision(df_test[\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Precision, Male:\":<20}{getPrecision(df_test[df_test[\"Sex_F\"]==0][\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Precision, Female:\":<20}{getPrecision(df_test[df_test[\"Sex_F\"]==1][\"CM_Val\"]):.2f}')\n",
    "  print()\n",
    "  print(f'{\"Accuracy:\":<20}{getAccuracy(df_test[\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Accuracy, Male:\":<20}{getAccuracy(df_test[df_test[\"Sex_F\"]==0][\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Accuracy, Female:\":<20}{getAccuracy(df_test[df_test[\"Sex_F\"]==1][\"CM_Val\"]):.2f}')\n",
    "  print()\n",
    "  print(f'{\"Recall:\":<20}{getRecall(df_test[\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Recall, Male:\":<20}{getRecall(df_test[df_test[\"Sex_F\"]==0][\"CM_Val\"]):.2f}')\n",
    "  print(f'{\"Recall, Female:\":<20}{getRecall(df_test[df_test[\"Sex_F\"]==1][\"CM_Val\"]):.2f}')\n",
    "\n",
    "  print(liner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/OAS2_nii/test/class_1/OAS2_0002_MR2_V2.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_p \u001b[38;5;241m=\u001b[39m \u001b[43mgetModelPredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[102], line 6\u001b[0m, in \u001b[0;36mgetModelPredictions\u001b[0;34m(model, df, columnName)\u001b[0m\n\u001b[1;32m      3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df_test\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 6\u001b[0m   img \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m   img_arr \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimg_to_array(img)\n\u001b[1;32m      8\u001b[0m   img_arr \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img_arr, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bmen-619-oasis-2/venv/lib/python3.10/site-packages/keras/src/utils/image_utils.py:235\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, pathlib\u001b[38;5;241m.\u001b[39mPath):\n\u001b[1;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/OAS2_nii/test/class_1/OAS2_0002_MR2_V2.jpg'"
     ]
    }
   ],
   "source": [
    "df_p = getModelPredictions(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "Precision:          0.79\n",
      "Precision, Male:    0.80\n",
      "Precision, Female:  0.75\n",
      "\n",
      "Accuracy:           0.74\n",
      "Accuracy, Male:     0.74\n",
      "Accuracy, Female:   0.75\n",
      "\n",
      "Recall:             0.31\n",
      "Recall, Male:       0.57\n",
      "Recall, Female:     0.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "printModelMetrics(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
